<!DOCTYPE html>
<html lang="en">
  
  {% extends "blog/base.html" %}

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Lessons learned from making an image dataset from scratch</title>

    <!-- Bootstrap core CSS -->
    <link href="/static/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="/static/vendor/font-awesome/css/font-awesome.min.css" rel="styles heet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="/static/css/clean-blog.min.css" rel="stylesheet">

  </head>

  <body>
    <!-- Page Header -->
    {% block headerContent %}
      <header class="masthead" style="background-image: url('/static/img/dataset_wallpaper.png')">
        <div class="overlay"></div>
        <div class="container">
          <div class="row">
            <div class="col-lg-8 col-md-10 mx-auto">
              <div class="post-heading">
                <h1>Lessons learned from making an image dataset from scratch</h1>
              </div>
            </div>
          </div>
        </div>
      </header>
    {% endblock %}

    <!-- Post Content -->
    {% block postContent %}
      <article>
        <div class="container">
          <div class="row">
            <div class="col-lg-8 col-md-10 mx-auto">
              <h2>Introduction</h2>
              <p>
                Modern<a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a> uses three main components to work.
                <ul>
                  <li>Large amounts of data</li>
                  <li>Machine/Deep learning</li>
                  <li>GPUs</li>
                </ul> 
                This recipe has proven to be effective and AI has become the topic of interest of many scientific and industry fields. For example, medicine uses AI to try to automate diagnostics and social media uses AI in recommendation systems. But this was not always the case, before 2008 AI was considered an almost dead topic. But in 2012, a laboratory in Toronto directed by Prof. Geofrey Hinton applied Deep learning to solve the Imagenet challenge and obtained results that surpassed the state-of-the-art in that year. Given the fact that the results were way better, Deep learning was retaken as the main method in AI. 
                As of today, 6 years after the resurgence of AI, it has been clear that the most difficult ingredient to collect has been data. Let me explain, suppose you want to make an translation system, it is achievable using sequence to sequence models. If your aspiration is to work on the project for learning purposes, then you are going to find a lot of data and resources online. But trouble starts when you want to apply this idea as a product. In this blog, I am going to tell you about the experience that I had about making an image dataset from scratch for the medical industry. Despite, I cannot tell you about the actual dataset I can describe the procedures I learned during the experience.

              </p>
              <h2>A little bit about products that use AI</h2>
              <p>
                When I first started building my product (which in the beginning was an university project) I did not realize how expensive it was going to be. At that time, 2014, we were working on automatic diagnostics for Tuberculosis. Data about the problem was impossible to find online. It meant we had to build the database and for that we required very expensive medical equipment. Me and my partner were at the university, so we tought it was going to be impossible to obtain the necessary equipment. Fortunately, we obtained money from an oversea contests and from the university. But, had I not been luck, I would not have been able to go forward. Anyways, datasets that power AI require an infraestructure that might be non-existant, very expensive or unfeasable to obtain. So, be sure to think ahead and if possible search for help. There is always a contest, hackathon, grant, etc. that might help you. 
              </p>
              <p>
                After completing the project in the university, we got a lot of publicity because of our results. We actually had a client and multiple offers from universities of Bolivia reached us in the hope that we would work with them. However, thankfully, we analised the situation and what would be the best next steps. Academia invitations were awaiting, but were those the ones that would allow us to create an Artificial Intelligence product? The answer was no because universities did not have access to datasets that would solve our problem. Besides, they surely would not invest large amounts of money as industry would. Thus, our conclusions moved our project to the industry where we would look for the necessary assets and investors. We managed to find an investor, but that is another story. The takeaway is that AI products require the ingredients mentioned in the introduction section, and if not obtained, AI will not work. So, think who is the best partner that would allow you to  solve the problem.
              </p>
              <h2>Where to start?</h2>
              <p>
                At this point you have the right partner, a good amount of money, access to the data and a MVP that proves your whole idea. What is the next step? 
                <ul>
                <li>Spend $5000 USD in gpus and cpus?</li>
                <li>Install deep learning libraries in your computer?</li>
                <li>Build your own dataset</li>
                </ul>
                Haha, so the answer should be clear. You should start building the dataset that will solve the problem.
              </p>
              <h2>Building your dataset</h2>
              <p>
                At first glance, this might sound challenging and it really is, but you have some options to alleviate your work. In the first place, given that you are in the business there might be an infraestructure that contains all the data you need. Let's say you are in a hospital, they have SQL databases that contain all the information about patients. It is actually mandatory for health centers to have backup of your patients, so you should be fine. Maybe the company does not have databases (which should be rarely and maybe ilegal), but they work with clients that provide the data. Then, your work should start on building the infraestructure to save that information. The third option is more applicable to startups or big companies that have contacts. In this case, your problem might even be so innovative that there is not really a record of that information. So, as a friend recommend me, you should look for a partner. Let's say that you want to build a clothes image classifier, then you should partner with a big retail store or something like that.   
              </p>
              <p>
                Let's define an example that we will use throughout this post. Maybe your big idea will be to create a mobile app that can detect the amount of calories that a particular food has.
              </p>
              <p>
                The concept that has to be clear at this stage is that building a dataset is like solving a Data Science problem. Here I will list the stages in Data Science.
                <ul>
                  <li>Define your problem</li>
                  <li>Data wrangling</li>
                  <li>Exploratory data analisis (EDA)</li>
                  <li>Conclusions</li>
                </ul>
                The next part of the blog focuses on building the dataset of the problem defined previously using Data Science.
              </p>
              <h2>Define your problem</h2>
              <p>
                This stage has two very important parameters: scalability and constraints. On one hand scalability is important because your dataset should be able to scale to deeper information, otherwise you might get stuck. On the other hand, constraints are also important because they are going to define what are going to be the limits to solve the problem.
              </p>
              <p>
                Let's define the dataset. It has to be scalable and constrained, a good practice is to build a tree using a top-down approach. On the root node of the tree, you will set the class food. The branches of this root node might be hard to identify. You could classify food by country, maybe cities, maybe types (spicy, sweet, etc). You have to make a good decision, usually the correct answer is found on papers. But, given that I doubt anyone would write about food, then you will have to work on it. In this case a good option would be simply define that each child root wil be a different food. E.g: Spaghetti, Lasagne, Fried chicken, others. The next nodes in the three are important as well, it would be good to define them as ingredients that make up the plate. E.g: chicken, pasta, sausage. We will leave the the nodes here since the problem is constrained enough and the data seems rather easy to collect, but note the problem is highly scalable. The next set of nodes can be sub-ingredients that make up the ingredients. It is important because our problem is to define the amount of calories of a particular food, not know where they come from or who created them.
              </p>
              <h2>Data wranlging</h2>
              <h3>Getting the data</h3>
              <p>We have defined how our dataset will look like. Now, let's get the data. As mentioned before, the easiest solution is to get the data from the company or find a partner. In the case none are available, it is our problem to obtain it.</p>
              <p>In our case, the data is available in multiple formats. I mean, you could just prepare every plate that you might think of and take multiple pictures of it. You could scrap Instagram or Facebook (which is ILEGAL). You could write down all the foods that you can think of (create a corpus) and then search them online (This might be ILEGAL as well since many pictures have copyrights). As I mentioned, the data is available but it is not legal to use. This is why a partner is always the best option. They will give you the rights to access the data.</p>
              <p>So, you might now realize that obtaining data is actually really hard. Of course, if your project is academic, then just ask for some permissions and most likely they will be granted for your project. But in the case of industry, the only viable option is a partnership or own te data.</p>
              <p>An important metric at this stage is to know when it is enough data. This is hard to define because in Data Science, the stages to solve the problem are linked to each other. E.g: if your EDA shows that you have a skew class, then you should go back to remove or get more data. So, experience counts at this point. In Deep Learning models, at least 5000 examples are expected per class. In the case of machine learning it might be more or less depending on the complexity of the problem. The best tip I have ever received to solve this is the following, "Don't be a hero, search the paper that solved the same or similar problem and start from there".</p>
              <h3>Cleaning the data</h3>
              <p>Assume you have obtained 5000 images of each class. That means you should have 50000 examples. I assure you not all of them are useful. Many of them are too noisy or are related to something else. Maybe When searching for hamburguers you accidentally queried images of restaurants that serve hamburguers. You need to clean this data. </p>
              <p>In the case of images this is really a painful job. It depends on the capacity of your labelers and their experience. If you are working on an easy project such as the food one, then you might use anybody to do this. But, if you are working in the medical field it will be costly and frustrating. Either case, the work has to be done and all images have to be correctly labeled. Have you ever heard the phrase "Garbage in, garbage out"? It is very important.</p>
              <h3>Converting to tidy data</h3>
              <p>This phase is easy. Your images might be stored in a SQL database whose table contains the path to the image, its associated label and maybe the bouding box that focuses the object. I don't know why, but many datasets have json files associated with each image. The json file contains information about the image, the bouding box and other characteristics. In my opinion, saving everything in a SQL database is the most organized way.</p>
              <h2>Exploratory data analysis</h2>
              <p>In Data Science, this is the fun part. Long hours looking for patterns, trying different things to get metrics and insights and plotting the data. </p>
              <p>However in the case of images, it is a bit different. Given the complexity of images, they are hard to analise. There are some methods, you could use PCA to extract features and compare classes. But that won't really tell you much, your human cognitive intelligence should be enough. In this type of problem you would do the following. Look at the balance of classes, that is counting the frequency of each class and checking if there are no skew classes. Query random images and check if there are differentiable patterns such as color, edges, shapes, contexts, etc. Check the variety of the examples in each class, this is to check that you have enough variety in your problem. Images of spaghetti at a fancy restaurant, at home, in the street, different lighting conditions, different preparations, different pasta, different parts of chicken.</p>
              <p>At this stage, I implement something called data augmentation. This is a set of techniques that will increase the size of your data set by creating more examples of a single data point. For example, rotating the image, shifting the colors, translating the object, cropping at different sizes, etc. I almost always use this technique to increase the size of my data because it always results in better performance. Howevery, you should be careful. Data augmentation requires critical thinking. I will explain, in the case you are trying to differentiate a grape from a peach, color should be really important. Therefore, you should not use color shifting or fancy PCA.</p>
              <h2>Conclusions</h2>
              <p>Once your metrics are validated. You can move onto making conclusions about your dataset. You can make a table that shows the amount of classes and the data distribution. You can show some images to show that variety is present and show how data augmentation creates new examples. These results will serve as an insight for the prediction phase.</p>
              <h2>Bonus</h2>
              In the case, you are at Silicon Valley a video might be sufficient, I mean, remeber the story of <a href="https://techcrunch.com/2011/10/19/dropbox-minimal-viable-product/">Dropbox</a>.
              <p>
                Before moving to the next part of the blog, I would like to discuss a little bit about what is in between the MVP and creating the dataset. You have to be very careful about using open source data to train AI. Suppose you are working on a self-driving-car, you would test your MVP using the COCO dataset. But releasing a product using this actual data is not allowed. Big datasets such as COCO, Imagenet and VOC do not grant permission for profitable products, services, etc. So, be careful.
              </p>
              <p>The previous pipeline is very effective, yet querying data might be time demanding. In this case, you can do as the people at Microsoft did with the COCO dataset. Divide the ultimate dataset in batches. For example, plan the dataset in three months of work, but each month will release a batch of data of N examples. Such that N+N+N makes the final dataset.</p>
              <h2>Summary</h2>
              <ul>
                <li> If you want to create a product that uses AI, then you <strong>should be related, work or be part</strong> of a some company/startup that has access to the type of data that describes your problem. Otherwise, it might be a waste of time because you are going to spend a lot of time just gathering data. </li>
                <li></li>
                <li></li>
                <li></li>
              </ul>
            </div>
          </div>
        </div>
      </article>
    {% endblock %}

    <!-- Bootstrap core JavaScript -->
    <script src="/static/vendor/jquery/jquery.min.js"></script>
    <script src="/static/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="/static/js/clean-blog.min.js"></script>

  </body>

</html>
